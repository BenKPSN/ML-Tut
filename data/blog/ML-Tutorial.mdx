---
title: A Machine Learning Tutorial
date: '2024-04-03'
tags: ['code', 'guide']
draft: false
summary: A Overview of Machine Learning Using Pytorch and Sklearn
---

## Introduction

    Pytorch and Sklearn are two open-source packages which contain a variety of tools to conduct data analysis and utilize machine learning.

This webpage will cover the process of utilizing these packages, as well as explaining the underlying calculations.

<TOCInline toc={props.toc} exclude="Introduction" />

## Setup

See previous paper for information on setup

## Model Classification

The **Sklearn** package provides a large variety of tools for the purposes of classification, regression, clustering, dimensionality reduction, etc.
It primarily cateogrizes models based on whether they are supervised or not. Some models of note include the following:

Supervised

- Linear Regression: A regression model which attempts to fit a linear model to the data. Requires independence of features.
- Logistic Regression: A classification model. In the multinomial case the model is capable of fitting up to an arbitrary K classes.
- Support Vector Machine (SVM): Used for classification, regression, and outlier detection. Effective in high dimensional spaces (many features).
- K-Nearest Neighbor: Used for classification and regression. Effective in cases where data is continuous
- Random Forest: Used for classification and regression. Combines mutiple decision trees to reduce variance.

Unsupervised

- K-Means: A clustering algorithim. Attempts to seperate data into an arbitrary N clusters
- Principle Component Analysis: Used for dimensionality reduction. Finds N orthogonal components that explain the maximum amount of variance.

Note that Sklearn provides too many models and tools to be listed here. For a full list see the following: https://scikit-learn.org/stable/supervised_learning.html

The **Pytorch** package provides the necessary tools to built a Neural Network layer by layer,
as such it is up to the user to determine what is necessary for the type of Neural Network they intend to design.
Neural Networks can be either fall under supervised or unsupervised learning based on whether or not the data is labeled.
Additionally, whether the model is used for classification or regression is also based on how the model is built.

## Linear Regression

### Parameters

Parameters of LinearRegression() are the following:

- fit_intercept (default: True): Whether to calculate an intercept for the model. If False the data is expected to be centered.
- copy_X (default: True): Creates a copy of the input X, otherwise data in X may be overwritten.
- n_jobs (default: None): Determines the maximum number of CPUs are used by default None = 1. If set to -1 all CPUs are used.
- positive (default: False): When True forces coefficients to be positive.

None of the parameters in this model require tuning.

### Example

```python
lindf = df.filter(df.columns[7:10]).dropna(how='any')
X = lindf.filter(lindf.columns[1:3])
y = lindf.iloc[:,0]
X
df.columns[10:13]
from sklearn.linear_model import LinearRegression
reg = LinearRegression().fit(X,y)
reg.score(X,y)
reg.predict([[103534,40261]])
```

## Logistic Regression

### Parameters

Parameters of interest for LogisticRegression() are the following:

- multi_class (default: ‘auto’): Defines whether the problem is binary. By default, this is
  automatically detected. To explicitly state the problem is binomial set to ‘ovr’, to state the
  problem is multinomial set as ‘multinomial’
- solver (default: ‘lbfgd’): defines the algorithm being used.
- max_iter (default: 1000): the maximum number of iterations for the solver to converge.
- random_state (default: None): Same as in test_train_split()

Note other parameters exist, but are not as important as the above.
For more information visit the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

In terms of parameter tuning the only parameter that may require tuning is "solver".
The supported solvers are as follows:

- liblinear: good for small datasets, limited to one-vs-rest schemes
- sag, saga: good for larger datasets, uses fast convergence, can handle multinomial loss.
- newton-cholesky: good for datasets with # of samples >> # of features.
- lbfgs, newton-cg: good for larger datasets, can handle multinomial loss.

### Example

```python
mlrdf = df.filter(data.columns[3:7])
mlrdf = mlrdf.dropna(how='any')
X = mlrdf.filter(mlrdf.columns[1:4])

# Factorize string to get values

y = pd.factorize(mlrdf.iloc[:,0])[0] + 1

# Import requires libraries

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size = 0.20, random_state = 5)

# if there is a problem with convergence try either normalizing data with sklearn's standardscaler or by increasing maxiter

model1 = LogisticRegression(random_state=0, multi_class='multinomial',
solver='lbfgs', max_iter=1000).fit(X_train, y_train)
preds = model1.predict(X_test)
```

## K-Nearest Neighbor Regressor

### Parameters

KNeighborsRegressor() defines how the K-Nearest Neighbor model is built.
Parameters of interest for KNeighborsRegressor() are the following:

- n_neighbors (default: 5): the number of neighbors being used.
- weights (default: ‘uniform’): when ‘uniform’ all points in a neighborhood are weighted equally. If set to ‘distance’ points are weight inversely to their distance.

Note other parameters exist, but are not as important as the above.
For more information visit the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html
In terms of parameter tuning the most sensitive parameters are "n_neighbors" and "weights".

- Using 'distance' instead of 'uniform' often leads to more jagged predictions when used with a low 'n_neighbors' value.
- The value of 'n-neighbor' should be determined based on how sparse the data is, with highly clustered data using higher values.

In terms of tuning these parameters, using a visualization to determine how predicted data point(s) fit can help determine a optimal number of 'n_neighbors' and 'distance'.

### Example

```python
from sklearn import neighbors
from sklearn.metrics import mean_squared_error
from math import sqrt
import matplotlib.pyplot as plt

rmse_val = [] #to store rmse values for different k

for K in range(20):
    K = K+1
    model = neighbors.KNeighborsRegressor(n_neighbors = K)

    model.fit(X_train, y_train)  #fit the model
    pred=model.predict(X_test) #make prediction on test set
    error = sqrt(mean_squared_error(y_test,pred)) #calculate rmse
    rmse_val.append(error) #store rmse values
    print('RMSE value for k= ' , K , 'is:', error)
```

## Random Forest

### Parameters

RandomForestRegressor() defines how the Random Forest Regressor model is built.
Parameters of interest for RandomForestRegressor() are the following:

- n_estimators (default: 100): the number of trees in the forest.

Note other parameters exist, but are not as important as the above.
For more information visit the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html

In terms of parameter tuning the most sensitive parameter is 'n_estimators'.

- Generally a model performance improves as the number of trees increase, but at the cost of computation speed. It is up to the user to determine an acceptable middle ground between computational speed and accuracy.

### Example

```python
import numpy as np

rfdf = pd.get_dummies(df = df.filter(df.columns[np.r_[0:6,10:25]]), columns = [df.columns[3]], drop_first=True, dtype=int)
rfdf = rfdf.dropna(how="any")
rfdf
# the value we want to predict
rfdf_label = np.array(pd.factorize(rfdf.iloc[:,1])[0]+1)
rfdf_label
rfdf_features = rfdf.filter(rfdf.columns[3:24])
rfdf_features["region"] = pd.factorize(rfdf.iloc[:,2])[0]+1
rfdf_features
feature_names = list(rfdf_features)
feature = np.array(rfdf_features)

train_features, test_features, train_labels, test_labels = train_test_split(feature, rfdf_label, test_size = 0.25, random_state = 42)

print('Training Features Shape:', train_features.shape)
print('Training Labels Shape:', train_labels.shape)
print('Testing Features Shape:', test_features.shape)
print('Testing Labels Shape:', test_labels.shape)
# Import the model we are using
from sklearn.ensemble import RandomForestRegressor
# Instantiate model with 1000 decision trees
rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)
# Train the model on training data
rf.fit(train_features, train_labels);
# Use the forest's predict method on the test data
predictions = rf.predict(test_features)
# Calculate the absolute errors
predictions
test_labels
errors = abs(predictions - test_labels)
# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')
errors
# Calculate mean absolute percentage error (MAPE)
mape = 100 * (errors / test_labels)
# Calculate and display accuracy
accuracy = 100 - np.mean(mape)
print('Accuracy:', round(accuracy, 2), '%.')
np.mean(mape)
```

## Neural Networks

### Parameters

DataLoader() prepares the data into batches that can be reshuffled at each epoch (one iteration of training).
Parameters of interest for DataLoader() are the following:

- dataset (required): the data set from which to load data.
- Batch_size (default: 1): The number of samples per batch.
- Shuffle (default: True): Set True to reshuffle at every epoch.
  nn.init.kaiming*uniform*() is one method of preparing the initial input tensors for the neural network to set initial
  weights. See https://pytorch.org/docs/stable/nn.init.html for alternative methods.
  Parameters of interest for nn.init.kaiming*uniform*() are the following:
- tensor (required): an n-dimensional tensor.
- nonlinearity (default: ‘leaky_relu’): Defines the activation function used. For this method the
  only options are ‘leaky_relu’ or ‘relu’ (note: ‘relu’ is short for ‘rectified linear’)
  nn.linear() applies a linear transformation on input data.
  Parameters of interest for nn.linear() are the following:
- in_features (required): size of input.
- out_features (required): size of output
  torch.nn.functional.relu(X) and torch.nn.functional.sigmoid(X) applies the rectified linear function/sigmoid
  function elementwise onto the tensor X.
  nn.MSELoss() is a loss function that computes the average of the squared differences between the actual and
  predicted values.
  Other loss functions of note include:
- nn.L1loss() which uses the average of absolute differences.
- nn.CrossEntropyLoss () which computes the difference of two probability distributions.
- See the following for a full list: https://pytorch.org/docs/stable/nn.html#loss-functions
  optim.SGD() implements stochastic gradient descent, an optimization algorithm that helps reduce the overall
  computation time.
  Parameters of interest for optim.SGD() are the following:
- params (required): parameters being optimized.
- lr (required): the learning rate.

Note other parameters exist, but are not as important as the above.
For more information visit the documentation: https://pytorch.org/docs/stable/index.html

In terms of parameter tuning the most sensitive parameters are 'lr', the number of epochs (the number of times the optimizer is run), the number of layers and 'in_features'/'out_features' (the number of nodes per layer).

- 'lr' or learning rate determines the step size taken by the optimizer, increasing it can lead to faster convergence but may cause overfitting.
- More epochs will similarly improve performance, but can also cause overfitting
- number of layers determines the complexity and ability for the model to learn.
- number of nodes per layer determine the layer's capacity to represent relationships between nodes.

To optimize these parameters it is suggested you use a hyperparameter tuning technique:

- Bayesian Optimization: builds a probability model of the objective function and uses it to evaluate the hyperparameters.
- Grid Search: Uses a grid containing a list of hyperparameters and a performance meteric going through each combination to determine what performs the best.
- Random Search: Similar to grid search, but instead of every combination a random subset of combinations are examined.
- Manual: Using a performance meteric, manually edit the hyperparameters until performance is optimized.

### Example

```python
import torch
from torch.utils.data import Dataset, DataLoader
from torch import nn
from torch import optim
# Define class that converts data to torch tensors
class Data(Dataset):
 def __init__(self, X, y):
 self.X = torch.from_numpy(X.astype(np.float32))
 self.y = torch.from_numpy(y.astype(np.float32))
 self.len = self.X.shape[0]

 def __getitem__(self, index):
 return self.X[index], self.y[index]

 def __len__(self):
 return self.len

batch_size = 16
# Prepare testing and training data for input into the neural network (NN)
train_data = Data(X_train, y_train)
train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)
test_data = Data(X_test, y_test)
test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)
input_dim = 21
hidden_dim = 100
output_dim = 166
# Define the NN
class NeuralNetwork(nn.Module):
 def __init__(self, input_dim, hidden_dim, output_dim):
 super(NeuralNetwork, self).__init__()
 self.layer_1 = nn.Linear(input_dim, hidden_dim)
 nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity="relu")
 self.layer_2 = nn.Linear(hidden_dim, output_dim)

 def forward(self, x):
 x = torch.nn.functional.relu(self.layer_1(x))
 x = torch.nn.functional.sigmoid(self.layer_2(x))
 return x
# Initialize NN
model = NeuralNetwork(input_dim, hidden_dim, output_dim)
# Setup Loss function and Stochastic Gradient Descent function
loss_fn = nn. MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
num_epochs = 100
loss_values = []
for epoch in range(num_epochs):
 for X, y in train_dataloader:
 # zero the parameter gradients
 optimizer.zero_grad()

 # forward + backward + optimize
 pred = model(X)
 loss = loss_fn(pred, y.type(torch.LongTensor))
 loss_values.append(loss.item())
 loss.backward()
 optimizer.step()
```

## References

Data Used: https://apps.who.int/nha/database/Select/Indicators/en

Model documentation: https://scikit-learn.org/
Neural Network documentation: https://pytorch.org/docs/stable/index.html
Model Classification: https://www.datacamp.com/blog/classification-machine-learning
NN Hyperparameter Tuning: https://aws.amazon.com/what-is/hyperparameter-tuning
Random Forest: https://builtin.com/data-science/random-forest-algorithm
